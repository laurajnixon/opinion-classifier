{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5NTVrABy-vH"
      },
      "source": [
        "#### Mount drive, import libraries, models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZtVFLaIy2po"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZe2Z_JEgTMa"
      },
      "outputs": [],
      "source": [
        "!pip install spacy~=2.0\n",
        "!spacy download en_core_web_sm\n",
        "!pip install transformers==4.28.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGwSps22nfim"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "import transformers\n",
        "import joblib\n",
        "\n",
        "#from sentence_transformers import SentenceTransformer\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoTokenizer, AutoModel, BertModel, BertTokenizer, BertTokenizerFast\n",
        "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\n",
        "from transformers.models.bert.modeling_bert import BertForSequenceClassification\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnvbwYSK5WF3"
      },
      "source": [
        "#### Import and process news article dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FgVDJNWx1CfI"
      },
      "outputs": [],
      "source": [
        "#Import news article data json file (test set already set aside)\n",
        "news_json = pd.read_json('/content/drive/My Drive/Data science/GNI87-json.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1AbUZ5uvP2bf"
      },
      "outputs": [],
      "source": [
        "#Rename json file\n",
        "df = news_json\n",
        "\n",
        "#Import df of article metadata w/news_opinion labels\n",
        "label_df = pd.read_csv(\"/content/drive/My Drive/Data science/All_Metadata.csv\")\n",
        "\n",
        "#Create dict of article IDs/news_opinion labels, map to full text df, and drop unlabelled articles and duplicates\n",
        "label_dict = dict(zip(label_df['Article ID'], label_df['newsop']))\n",
        "\n",
        "df[\"newsop\"] = df[\"Article ID\"].map(label_dict)\n",
        "\n",
        "#Drop records with no label\n",
        "df = df[-df[\"newsop\"].isnull()].copy()\n",
        "\n",
        "df = df.drop_duplicates('Article ID').drop_duplicates([\"Media Name\", \"Body\"])\n",
        "\n",
        "#Convert labels to numeric format\n",
        "df['target'] = np.where(df.newsop == \"Opinion\", 1, 0)\n",
        "\n",
        "print(len(df))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYnpD9vmzrmZ"
      },
      "source": [
        "#### Create test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SgPz8RDDzxdn"
      },
      "outputs": [],
      "source": [
        "newsdf = df[df.newsop == 'News'].sample(n=10500, random_state=42).copy()\n",
        "opdf = df[df.newsop == 'Opinion'].sample(n=10500, random_state=42).copy()\n",
        "devdf = pd.concat([newsdf, opdf])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wx3u3CgC0neP"
      },
      "outputs": [],
      "source": [
        "#Select only articles not used in development\n",
        "dev_IDs = devdf['Article ID'].to_list()\n",
        "testdf = df[-df['Article ID'].isin(dev_IDs)]\n",
        "\n",
        "#Create balanced 10% sample\n",
        "testnewsdf = testdf[testdf.newsop == 'News'].sample(n=2500, random_state=42).copy()\n",
        "testopdf = testdf[testdf.newsop == 'Opinion'].sample(n=2500, random_state=42).copy()\n",
        "testdf = pd.concat([testnewsdf, testopdf]).copy()\n",
        "\n",
        "#Remove test set from overall dataset\n",
        "test_IDs = testdf['Article ID'].to_list()\n",
        "df = df[-df['Article ID'].isin(test_IDs)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzgAmiy7zddK"
      },
      "source": [
        "#### Create training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "okHnf7sFo69D"
      },
      "outputs": [],
      "source": [
        "newsdf = df[df.newsop == 'News'].sample(n=7500, random_state=42).copy()\n",
        "opdf = df[df.newsop == 'Opinion'].sample(n=7500, random_state=42).copy()\n",
        "df = pd.concat([newsdf, opdf])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ml8Zd2doiPmj"
      },
      "outputs": [],
      "source": [
        "len(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlGkiGNzI4zu"
      },
      "source": [
        "#### Strip leading metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ClqHzlv-jy6b"
      },
      "outputs": [],
      "source": [
        "#Strip Factiva style metadata pattern from training data\n",
        "df[\"Body_clean\"] = df[\"Body\"].replace(r'(?:.*\\s+)?Media: .*\\s+(?:Byline|Author): .*\\s+Date: .*\\n' ,'', regex=True)\n",
        "\n",
        "#Strip Nexis style metadata pattern\n",
        "df['body_clean'] = np.where(df['Body_clean'].str.contains('BYLINE: '), df['Body_clean'].str.split('BYLINE: .*', regex=True, expand=True)[1], df['Body_clean'])\n",
        "df['body_clean'] = np.where(df['body_clean'].str.contains('SECTION: '), df['body_clean'].str.split('SECTION: .*', regex=True, expand=True)[1], df['body_clean'])\n",
        "df['body_clean'] = np.where(df['body_clean'].str.contains('LENGTH: '), df['body_clean'].str.split('LENGTH: .*', regex=True, expand=True)[1], df['body_clean'])\n",
        "df['body_clean'] = np.where(df['body_clean'].str.contains('DATELINE: '), df['body_clean'].str.split('DATELINE: .*', regex=True, expand=True)[1], df['body_clean'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vyIXqlv8rvxu"
      },
      "outputs": [],
      "source": [
        "#Strip Factiva style metadata pattern from test data\n",
        "testdf[\"Body_clean\"] = testdf[\"Body\"].replace(r'(?:.*\\s+)?Media: .*\\s+(?:Byline|Author): .*\\s+Date: .*\\n' ,'', regex=True)\n",
        "\n",
        "#Strip Nexis style metadata pattern\n",
        "testdf['body_clean'] = np.where(testdf['Body_clean'].str.contains('BYLINE: '), testdf['Body_clean'].str.split('BYLINE: .*', regex=True, expand=True)[1], testdf['Body_clean'])\n",
        "testdf['body_clean'] = np.where(testdf['body_clean'].str.contains('SECTION: '), testdf['body_clean'].str.split('SECTION: .*', regex=True, expand=True)[1], testdf['body_clean'])\n",
        "testdf['body_clean'] = np.where(testdf['body_clean'].str.contains('LENGTH: '), testdf['body_clean'].str.split('LENGTH: .*', regex=True, expand=True)[1], testdf['body_clean'])\n",
        "testdf['body_clean'] = np.where(testdf['body_clean'].str.contains('DATELINE: '), testdf['body_clean'].str.split('DATELINE: .*', regex=True, expand=True)[1], testdf['body_clean'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itMzBDzUy2GB"
      },
      "source": [
        "#### Create input strings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dxde2QXazAbv"
      },
      "outputs": [],
      "source": [
        "df['start_end'] = np.where(df.body_clean.str.len() > 2000,\n",
        "             df.body_clean.str[0:1000].fillna(' ').copy() + '\\n*****\\n' + df.body_clean.str[-1000:].fillna(' ').copy(),\n",
        "             df.body_clean)\n",
        "\n",
        "testdf['start_end'] = np.where(testdf.body_clean.str.len() > 2000,\n",
        "             testdf.body_clean.str[0:1000].fillna(' ').copy() + '\\n*****\\n' + testdf.body_clean.str[-1000:].fillna(' ').copy(),\n",
        "             testdf.body_clean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZzQ8hq35zAep"
      },
      "outputs": [],
      "source": [
        "df['metadata'] = df['Headline'].fillna(' ') + '\\n*****\\n' + df['Media Name'].fillna(' ') + '\\n*****\\n' + df['Journalist Name'].fillna(' ')\n",
        "\n",
        "testdf['metadata'] = testdf['Headline'].fillna(' ') + '\\n*****\\n' + testdf['Media Name'].fillna(' ') + '\\n*****\\n' + testdf['Journalist Name'].fillna(' ')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1H_HZEK4-XxI"
      },
      "source": [
        "#### Create labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rMV8WZTd9apI"
      },
      "outputs": [],
      "source": [
        "y_train = df['target'].copy()\n",
        "y_test = testdf['target'].copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxHP9ZP1Fpob"
      },
      "source": [
        "#### Define Dataset/helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CKAyw0N9nODi"
      },
      "outputs": [],
      "source": [
        "class MyDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BnclKi3OnOk5"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    pre = precision_score(labels, preds)\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    rec = recall_score(labels, preds)\n",
        "    f1 = f1_score(labels, preds)\n",
        "    return {\n",
        "      'precision_score': pre,\n",
        "      'accuracy_score': acc,\n",
        "        \"recall_score\": rec,\n",
        "        \"f1_score\": f1\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Experiments"
      ],
      "metadata": {
        "id": "9Cb8rrraiEdH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxD-BdsvxsKE"
      },
      "source": [
        "#### Start of article"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NDLUVFcCxsKG"
      },
      "outputs": [],
      "source": [
        "X_train = df.body_clean.fillna(' ').copy()\n",
        "X_test = testdf.body_clean.fillna(' ').copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rp5h8fw9xsKH"
      },
      "outputs": [],
      "source": [
        "model_name = 'bert-base-cased'\n",
        "max_length = 512\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "train_encodings = tokenizer(X_train.tolist(), truncation=True, padding=True, max_length=max_length, return_attention_mask=True)\n",
        "test_encodings  = tokenizer(X_test.tolist(), truncation=True, padding=True, max_length=max_length, return_attention_mask=True)\n",
        "\n",
        "train_dataset = MyDataset(train_encodings, y_train.tolist())\n",
        "test_dataset = MyDataset(test_encodings, y_test.tolist())\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2).to(device)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    num_train_epochs=3,              # total number of training epochs\n",
        "    per_device_train_batch_size=16,  # batch size per device during training\n",
        "    per_device_eval_batch_size=16,   # batch size for evaluation\n",
        "    learning_rate=2e-5,              # initial learning rate for Adam optimizer\n",
        "    warmup_steps=50,                # number of warmup steps for learning rate scheduler (set lower because of small dataset size)\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    output_dir='results',          # output directory\n",
        "    logging_dir='logs',            # directory for storing logs\n",
        "    logging_steps=100,               # number of steps to output logging (set lower because of small dataset size)\n",
        "    evaluation_strategy='steps',     # evaluate during fine-tuning so that we can see progress\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=train_dataset,         # training dataset\n",
        "    eval_dataset=test_dataset,           # evaluation dataset (usually a validation set; here we just send our test set)\n",
        "    compute_metrics=compute_metrics      # our custom evaluation function\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1eAA4m8gxsKI"
      },
      "outputs": [],
      "source": [
        "trainer.train()\n",
        "results = trainer.evaluate()\n",
        "print(round(results['eval_f1_score'], 4))\n",
        "\n",
        "trainer.save_model('models/start')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arSxIIGFoTrb"
      },
      "source": [
        "#### Metadata + Start of article\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B85yOfDkoTrc"
      },
      "outputs": [],
      "source": [
        "X_train = df.metadata.copy() + df.body_clean.copy()\n",
        "X_test = testdf.metadata.copy() + testdf.body_clean.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tMSqE13roTrd"
      },
      "outputs": [],
      "source": [
        "#BERT base Headline tokenizations\n",
        "model_name = 'bert-base-cased'\n",
        "max_length = 512\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "train_encodings = tokenizer(X_train.tolist(), truncation=True, padding=True, max_length=max_length, return_attention_mask=True)\n",
        "test_encodings  = tokenizer(X_test.tolist(), truncation=True, padding=True, max_length=max_length, return_attention_mask=True)\n",
        "\n",
        "train_dataset = MyDataset(train_encodings, y_train.tolist())\n",
        "test_dataset = MyDataset(test_encodings, y_test.tolist())\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2).to(device)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    num_train_epochs=3,              # total number of training epochs\n",
        "    per_device_train_batch_size=16,  # batch size per device during training\n",
        "    per_device_eval_batch_size=16,   # batch size for evaluation\n",
        "    learning_rate=2e-5,              # initial learning rate for Adam optimizer\n",
        "    warmup_steps=50,                # number of warmup steps for learning rate scheduler (set lower because of small dataset size)\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    output_dir='results',          # output directory\n",
        "    logging_dir='logs',            # directory for storing logs\n",
        "    logging_steps=100,               # number of steps to output logging (set lower because of small dataset size)\n",
        "    evaluation_strategy='steps',     # evaluate during fine-tuning so that we can see progress\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=train_dataset,         # training dataset\n",
        "    eval_dataset=test_dataset,           # evaluation dataset (usually a validation set; here we just send our test set)\n",
        "    compute_metrics=compute_metrics      # our custom evaluation function\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_v-6RbJoTre"
      },
      "outputs": [],
      "source": [
        "trainer.train()\n",
        "results = trainer.evaluate()\n",
        "print(round(results['eval_f1_score'], 4))\n",
        "\n",
        "trainer.save_model('models/start_metadata')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1n0H6HX90jw9"
      },
      "source": [
        "#### Start of article + End of article"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NU6i-LkA0jw-"
      },
      "outputs": [],
      "source": [
        "X_train = df['start_end'].copy()\n",
        "X_test = testdf['start_end'].copy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nDGb2N4T0jxA"
      },
      "outputs": [],
      "source": [
        "model_name = 'bert-base-cased'\n",
        "max_length = 512\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "train_encodings = tokenizer(X_train.tolist(), truncation=True, padding=True, max_length=max_length, return_attention_mask=True)\n",
        "test_encodings  = tokenizer(X_test.tolist(), truncation=True, padding=True, max_length=max_length, return_attention_mask=True)\n",
        "\n",
        "train_dataset = MyDataset(train_encodings, y_train.tolist())\n",
        "test_dataset = MyDataset(test_encodings, y_test.tolist())\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2).to(device)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    num_train_epochs=3,              # total number of training epochs\n",
        "    per_device_train_batch_size=16,  # batch size per device during training\n",
        "    per_device_eval_batch_size=16,   # batch size for evaluation\n",
        "    learning_rate=2e-5,              # initial learning rate for Adam optimizer\n",
        "    warmup_steps=50,                # number of warmup steps for learning rate scheduler (set lower because of small dataset size)\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    output_dir='results',          # output directory\n",
        "    logging_dir='logs',            # directory for storing logs\n",
        "    logging_steps=100,               # number of steps to output logging (set lower because of small dataset size)\n",
        "    evaluation_strategy='steps',     # evaluate during fine-tuning so that we can see progress\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=train_dataset,         # training dataset\n",
        "    eval_dataset=test_dataset,           # evaluation dataset (usually a validation set; here we just send our test set)\n",
        "    compute_metrics=compute_metrics      # our custom evaluation function\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-6lENf6m0jxB"
      },
      "outputs": [],
      "source": [
        "trainer.train()\n",
        "results = trainer.evaluate()\n",
        "print(round(results['eval_f1_score'], 4))\n",
        "\n",
        "trainer.save_model('models/start_end')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMFQ5Mm8mTS1"
      },
      "source": [
        "#### Metadata + Start of article + End of article"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WOwfjk7VmTS3"
      },
      "outputs": [],
      "source": [
        "X_train = df['metadata'].copy() + '\\n****\\n' + df.start_end.copy()\n",
        "X_test = testdf['metadata'].copy() + '\\n****\\n' + testdf.start_end.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cfB4eu7MmTS4"
      },
      "outputs": [],
      "source": [
        "#BERT base Headline tokenizations\n",
        "model_name = 'bert-base-cased'\n",
        "max_length = 512\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "train_encodings = tokenizer(X_train.tolist(), truncation=True, padding=True, max_length=max_length, return_attention_mask=True)\n",
        "test_encodings  = tokenizer(X_test.tolist(), truncation=True, padding=True, max_length=max_length, return_attention_mask=True)\n",
        "\n",
        "train_dataset = MyDataset(train_encodings, y_train.tolist())\n",
        "test_dataset = MyDataset(test_encodings, y_test.tolist())\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2).to(device)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    num_train_epochs=3,              # total number of training epochs\n",
        "    per_device_train_batch_size=16,  # batch size per device during training\n",
        "    per_device_eval_batch_size=16,   # batch size for evaluation\n",
        "    learning_rate=2e-5,              # initial learning rate for Adam optimizer\n",
        "    warmup_steps=50,                # number of warmup steps for learning rate scheduler (set lower because of small dataset size)\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    output_dir='results',          # output directory\n",
        "    logging_dir='logs',            # directory for storing logs\n",
        "    logging_steps=100,               # number of steps to output logging (set lower because of small dataset size)\n",
        "    evaluation_strategy='steps',     # evaluate during fine-tuning so that we can see progress\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=train_dataset,         # training dataset\n",
        "    eval_dataset=test_dataset,           # evaluation dataset (usually a validation set; here we just send our test set)\n",
        "    compute_metrics=compute_metrics      # our custom evaluation function\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-chrN6HfmTS6"
      },
      "outputs": [],
      "source": [
        "trainer.train()\n",
        "results = trainer.evaluate()\n",
        "print(round(results['eval_f1_score'], 4))\n",
        "\n",
        "trainer.save_model('models/metadata_start_end')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzSV3v1pljze"
      },
      "source": [
        "#### End of article + Metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "rMcicCi4ljzg"
      },
      "outputs": [],
      "source": [
        "X_train = df.Body.fillna(' ').copy() + '\\n*****\\n' + df['metadata'].copy()\n",
        "X_test = testdf.Body.fillna(' ').copy() + '\\n*****\\n' + testdf['metadata'].copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "fV88oM8Lljzh"
      },
      "outputs": [],
      "source": [
        "#BERT base Headline tokenizations\n",
        "model_name = 'bert-base-cased'\n",
        "max_length = 512\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name, truncation_side='left')\n",
        "train_encodings = tokenizer(X_train.tolist(), truncation=True, padding=True, max_length=max_length, return_attention_mask=True)\n",
        "test_encodings  = tokenizer(X_test.tolist(), truncation=True, padding=True, max_length=max_length, return_attention_mask=True)\n",
        "\n",
        "train_dataset = MyDataset(train_encodings, y_train.tolist())\n",
        "test_dataset = MyDataset(test_encodings, y_test.tolist())\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2).to(device)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    num_train_epochs=3,              # total number of training epochs\n",
        "    per_device_train_batch_size=16,  # batch size per device during training\n",
        "    per_device_eval_batch_size=16,   # batch size for evaluation\n",
        "    learning_rate=2e-5,              # initial learning rate for Adam optimizer\n",
        "    warmup_steps=50,                # number of warmup steps for learning rate scheduler (set lower because of small dataset size)\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    output_dir='results',          # output directory\n",
        "    logging_dir='logs',            # directory for storing logs\n",
        "    logging_steps=100,               # number of steps to output logging (set lower because of small dataset size)\n",
        "    evaluation_strategy='steps',     # evaluate during fine-tuning so that we can see progress\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=train_dataset,         # training dataset\n",
        "    eval_dataset=test_dataset,           # evaluation dataset (usually a validation set; here we just send our test set)\n",
        "    compute_metrics=compute_metrics      # our custom evaluation function\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "leR0xS0Bljzj"
      },
      "outputs": [],
      "source": [
        "trainer.train()\n",
        "results = trainer.evaluate()\n",
        "print(round(results['eval_f1_score'], 4))\n",
        "\n",
        "trainer.save_model('models/end_metadata')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVSZrGOMeAsm"
      },
      "source": [
        "#### End of article"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4Dw5CH6neAsn"
      },
      "outputs": [],
      "source": [
        "X_train = df.Body.fillna(' ').copy()\n",
        "X_test = testdf.Body.fillna(' ').copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ias-IekJeAso"
      },
      "outputs": [],
      "source": [
        "#BERT base Headline tokenizations\n",
        "model_name = 'bert-base-cased'\n",
        "max_length = 512\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name, truncation_side='left')\n",
        "train_encodings = tokenizer(X_train.tolist(), truncation=True, padding=True, max_length=max_length, return_attention_mask=True)\n",
        "test_encodings  = tokenizer(X_test.tolist(), truncation=True, padding=True, max_length=max_length, return_attention_mask=True)\n",
        "\n",
        "train_dataset = MyDataset(train_encodings, y_train.tolist())\n",
        "test_dataset = MyDataset(test_encodings, y_test.tolist())\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2).to(device)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    num_train_epochs=3,              # total number of training epochs\n",
        "    per_device_train_batch_size=16,  # batch size per device during training\n",
        "    per_device_eval_batch_size=16,   # batch size for evaluation\n",
        "    learning_rate=2e-5,              # initial learning rate for Adam optimizer\n",
        "    warmup_steps=50,                # number of warmup steps for learning rate scheduler (set lower because of small dataset size)\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    output_dir='results',          # output directory\n",
        "    logging_dir='logs',            # directory for storing logs\n",
        "    logging_steps=100,               # number of steps to output logging (set lower because of small dataset size)\n",
        "    evaluation_strategy='steps',     # evaluate during fine-tuning so that we can see progress\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=train_dataset,         # training dataset\n",
        "    eval_dataset=test_dataset,           # evaluation dataset (usually a validation set; here we just send our test set)\n",
        "    compute_metrics=compute_metrics      # our custom evaluation function\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "o-BaHQ7LeAsp"
      },
      "outputs": [],
      "source": [
        "trainer.train()\n",
        "results = trainer.evaluate()\n",
        "print(round(results['eval_f1_score'], 4))\n",
        "\n",
        "trainer.save_model('models/end')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktT5DBFXbMRn"
      },
      "source": [
        "#### Metadata, max_length=128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6RmlamfNbMRo"
      },
      "outputs": [],
      "source": [
        "X_train = df.metadata.fillna(' ').copy()\n",
        "X_test = testdf.metadata.fillna(' ').copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wgl8eZYNbMRp"
      },
      "outputs": [],
      "source": [
        "#BERT base Headline tokenizations\n",
        "model_name = 'bert-base-cased'\n",
        "max_length = 128\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "train_encodings = tokenizer(X_train.tolist(), truncation=True, padding=True, max_length=max_length, return_attention_mask=True)\n",
        "test_encodings  = tokenizer(X_test.tolist(), truncation=True, padding=True, max_length=max_length, return_attention_mask=True)\n",
        "\n",
        "train_dataset = MyDataset(train_encodings, y_train.tolist())\n",
        "test_dataset = MyDataset(test_encodings, y_test.tolist())\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2).to(device)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    num_train_epochs=3,              # total number of training epochs\n",
        "    per_device_train_batch_size=16,  # batch size per device during training\n",
        "    per_device_eval_batch_size=16,   # batch size for evaluation\n",
        "    learning_rate=2e-5,              # initial learning rate for Adam optimizer\n",
        "    warmup_steps=50,                # number of warmup steps for learning rate scheduler (set lower because of small dataset size)\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    output_dir='results',          # output directory\n",
        "    logging_dir='logs',            # directory for storing logs\n",
        "    logging_steps=100,               # number of steps to output logging (set lower because of small dataset size)\n",
        "    evaluation_strategy='steps',     # evaluate during fine-tuning so that we can see progress\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=train_dataset,         # training dataset\n",
        "    eval_dataset=test_dataset,           # evaluation dataset (usually a validation set; here we just send our test set)\n",
        "    compute_metrics=compute_metrics      # our custom evaluation function\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96k03SzrbMRq"
      },
      "outputs": [],
      "source": [
        "trainer.train()\n",
        "results = trainer.evaluate()\n",
        "print(round(results['eval_f1_score'], 4))\n",
        "\n",
        "trainer.save_model('models/metadata')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPx_lhowYFk6"
      },
      "source": [
        "#### Headline, max_length=64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ouiDFQnTYFk7"
      },
      "outputs": [],
      "source": [
        "#Pre-process Headline text using texthero\n",
        "X_train = df['Headline'].fillna('').copy()\n",
        "X_test = testdf['Headline'].fillna('').copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "j7mwQkdxYFk8"
      },
      "outputs": [],
      "source": [
        "#BERT base Headline tokenizations\n",
        "model_name = 'bert-base-cased'\n",
        "max_length = 64\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "train_encodings = tokenizer(X_train.tolist(), truncation=True, padding=True, max_length=max_length, return_attention_mask=True)\n",
        "test_encodings  = tokenizer(X_test.tolist(), truncation=True, padding=True, max_length=max_length, return_attention_mask=True)\n",
        "\n",
        "train_dataset = MyDataset(train_encodings, y_train.tolist())\n",
        "test_dataset = MyDataset(test_encodings, y_test.tolist())\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2).to(device)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    num_train_epochs=3,              # total number of training epochs\n",
        "    per_device_train_batch_size=16,  # batch size per device during training\n",
        "    per_device_eval_batch_size=16,   # batch size for evaluation\n",
        "    learning_rate=2e-5,              # initial learning rate for Adam optimizer\n",
        "    warmup_steps=50,                # number of warmup steps for learning rate scheduler (set lower because of small dataset size)\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    output_dir='results',          # output directory\n",
        "    logging_dir='logs',            # directory for storing logs\n",
        "    logging_steps=100,               # number of steps to output logging (set lower because of small dataset size)\n",
        "    evaluation_strategy='steps',     # evaluate during fine-tuning so that we can see progress\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=train_dataset,         # training dataset\n",
        "    eval_dataset=test_dataset,           # evaluation dataset (usually a validation set; here we just send our test set)\n",
        "    compute_metrics=compute_metrics      # our custom evaluation function\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lGoE75yFYFk9"
      },
      "outputs": [],
      "source": [
        "trainer.train()\n",
        "results = trainer.evaluate()\n",
        "print(round(results['eval_f1_score'], 4))\n",
        "\n",
        "trainer.save_model('models/headline')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HbLlFUhaupfa"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RpBO9PnnupiW"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuClass": "premium"
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}