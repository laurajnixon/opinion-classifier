{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5NTVrABy-vH"
      },
      "source": [
        "#### Mount drive, import libraries, models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZtVFLaIy2po",
        "outputId": "019a2508-8d90-4843-eb60-b9e6f6e1a623"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZe2Z_JEgTMa"
      },
      "outputs": [],
      "source": [
        "!pip install spacy~=2.0\n",
        "!spacy download en_core_web_sm\n",
        "!pip install transformers==4.28.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGwSps22nfim"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "import transformers\n",
        "import joblib\n",
        "\n",
        "#from sentence_transformers import SentenceTransformer\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoTokenizer, AutoModel, BertModel, BertTokenizer, BertTokenizerFast\n",
        "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\n",
        "from transformers.models.bert.modeling_bert import BertForSequenceClassification\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnvbwYSK5WF3"
      },
      "source": [
        "#### Import and process news article dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FgVDJNWx1CfI"
      },
      "outputs": [],
      "source": [
        "#Import news article data json file (test set already set aside)\n",
        "news_json = pd.read_json('/content/drive/My Drive/Data science/GNI87-json.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1AbUZ5uvP2bf"
      },
      "outputs": [],
      "source": [
        "#Rename json file\n",
        "df = news_json\n",
        "\n",
        "#Import df of article metadata w/news_opinion labels\n",
        "label_df = pd.read_csv(\"/content/drive/My Drive/Data science/All_Metadata.csv\")\n",
        "\n",
        "#Create dict of article IDs/news_opinion labels, map to full text df, and drop unlabelled articles and duplicates\n",
        "label_dict = dict(zip(label_df['Article ID'], label_df['newsop']))\n",
        "\n",
        "df[\"newsop\"] = df[\"Article ID\"].map(label_dict)\n",
        "\n",
        "#Drop records with no label\n",
        "df = df[-df[\"newsop\"].isnull()].copy()\n",
        "\n",
        "df = df.drop_duplicates('Article ID').drop_duplicates([\"Media Name\", \"Body\"])\n",
        "\n",
        "#Convert labels to numeric format\n",
        "df['target'] = np.where(df.newsop == \"Opinion\", 1, 0)\n",
        "\n",
        "print(len(df))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYnpD9vmzrmZ"
      },
      "source": [
        "#### Create test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SgPz8RDDzxdn"
      },
      "outputs": [],
      "source": [
        "newsdf = df[df.newsop == 'News'].sample(n=10500, random_state=42).copy()\n",
        "opdf = df[df.newsop == 'Opinion'].sample(n=10500, random_state=42).copy()\n",
        "devdf = pd.concat([newsdf, opdf])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wx3u3CgC0neP"
      },
      "outputs": [],
      "source": [
        "#Select only articles not used in development\n",
        "dev_IDs = devdf['Article ID'].to_list()\n",
        "testdf = df[-df['Article ID'].isin(dev_IDs)]\n",
        "\n",
        "#Create balanced 10% sample\n",
        "testnewsdf = testdf[testdf.newsop == 'News'].sample(n=2500, random_state=42).copy()\n",
        "testopdf = testdf[testdf.newsop == 'Opinion'].sample(n=2500, random_state=42).copy()\n",
        "testdf = pd.concat([testnewsdf, testopdf]).copy()\n",
        "\n",
        "#Remove test set from overall dataset\n",
        "test_IDs = testdf['Article ID'].to_list()\n",
        "df = df[-df['Article ID'].isin(test_IDs)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzgAmiy7zddK"
      },
      "source": [
        "#### Create training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "okHnf7sFo69D"
      },
      "outputs": [],
      "source": [
        "newsdf = df[df.newsop == 'News'].sample(n=7500, random_state=42).copy()\n",
        "opdf = df[df.newsop == 'Opinion'].sample(n=7500, random_state=42).copy()\n",
        "df = pd.concat([newsdf, opdf])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ml8Zd2doiPmj",
        "outputId": "9c54a0ef-64fd-44da-e0af-3a5d43271732"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15000"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "len(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlGkiGNzI4zu"
      },
      "source": [
        "#### Strip leading metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ClqHzlv-jy6b"
      },
      "outputs": [],
      "source": [
        "#Strip Factiva style metadata pattern from training data\n",
        "df[\"Body_clean\"] = df[\"Body\"].replace(r'(?:.*\\s+)?Media: .*\\s+(?:Byline|Author): .*\\s+Date: .*\\n' ,'', regex=True)\n",
        "\n",
        "#Strip Nexis style metadata pattern\n",
        "df['body_clean'] = np.where(df['Body_clean'].str.contains('BYLINE: '), df['Body_clean'].str.split('BYLINE: .*', regex=True, expand=True)[1], df['Body_clean'])\n",
        "df['body_clean'] = np.where(df['body_clean'].str.contains('SECTION: '), df['body_clean'].str.split('SECTION: .*', regex=True, expand=True)[1], df['body_clean'])\n",
        "df['body_clean'] = np.where(df['body_clean'].str.contains('LENGTH: '), df['body_clean'].str.split('LENGTH: .*', regex=True, expand=True)[1], df['body_clean'])\n",
        "df['body_clean'] = np.where(df['body_clean'].str.contains('DATELINE: '), df['body_clean'].str.split('DATELINE: .*', regex=True, expand=True)[1], df['body_clean'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vyIXqlv8rvxu"
      },
      "outputs": [],
      "source": [
        "#Strip Factiva style metadata pattern from test data\n",
        "testdf[\"Body_clean\"] = testdf[\"Body\"].replace(r'(?:.*\\s+)?Media: .*\\s+(?:Byline|Author): .*\\s+Date: .*\\n' ,'', regex=True)\n",
        "\n",
        "#Strip Nexis style metadata pattern\n",
        "testdf['body_clean'] = np.where(testdf['Body_clean'].str.contains('BYLINE: '), testdf['Body_clean'].str.split('BYLINE: .*', regex=True, expand=True)[1], testdf['Body_clean'])\n",
        "testdf['body_clean'] = np.where(testdf['body_clean'].str.contains('SECTION: '), testdf['body_clean'].str.split('SECTION: .*', regex=True, expand=True)[1], testdf['body_clean'])\n",
        "testdf['body_clean'] = np.where(testdf['body_clean'].str.contains('LENGTH: '), testdf['body_clean'].str.split('LENGTH: .*', regex=True, expand=True)[1], testdf['body_clean'])\n",
        "testdf['body_clean'] = np.where(testdf['body_clean'].str.contains('DATELINE: '), testdf['body_clean'].str.split('DATELINE: .*', regex=True, expand=True)[1], testdf['body_clean'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itMzBDzUy2GB"
      },
      "source": [
        "#### Create input strings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dxde2QXazAbv"
      },
      "outputs": [],
      "source": [
        "df['start_end'] = np.where(df.body_clean.str.len() > 2000,\n",
        "             df.body_clean.str[0:1000].fillna(' ').copy() + '\\n*****\\n' + df.body_clean.str[-1000:].fillna(' ').copy(),\n",
        "             df.body_clean)\n",
        "\n",
        "testdf['start_end'] = np.where(testdf.body_clean.str.len() > 2000,\n",
        "             testdf.body_clean.str[0:1000].fillna(' ').copy() + '\\n*****\\n' + testdf.body_clean.str[-1000:].fillna(' ').copy(),\n",
        "             testdf.body_clean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZzQ8hq35zAep"
      },
      "outputs": [],
      "source": [
        "df['metadata'] = df['Headline'].fillna(' ') + '\\n*****\\n' + df['Media Name'].fillna(' ') + '\\n*****\\n' + df['Journalist Name'].fillna(' ')\n",
        "\n",
        "testdf['metadata'] = testdf['Headline'].fillna(' ') + '\\n*****\\n' + testdf['Media Name'].fillna(' ') + '\\n*****\\n' + testdf['Journalist Name'].fillna(' ')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1H_HZEK4-XxI"
      },
      "source": [
        "#### Create labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rMV8WZTd9apI"
      },
      "outputs": [],
      "source": [
        "y_train = df['target'].copy()\n",
        "y_test = testdf['target'].copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxHP9ZP1Fpob"
      },
      "source": [
        "#### Define Dataset/helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CKAyw0N9nODi"
      },
      "outputs": [],
      "source": [
        "class MyDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BnclKi3OnOk5"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    pre = precision_score(labels, preds)\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    rec = recall_score(labels, preds)\n",
        "    f1 = f1_score(labels, preds)\n",
        "    return {\n",
        "      'precision_score': pre,\n",
        "      'accuracy_score': acc,\n",
        "        \"recall_score\": rec,\n",
        "        \"f1_score\": f1\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Experiments"
      ],
      "metadata": {
        "id": "9Cb8rrraiEdH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxD-BdsvxsKE"
      },
      "source": [
        "#### Baseline experiment w/no preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NDLUVFcCxsKG"
      },
      "outputs": [],
      "source": [
        "X_train = df.body_clean.fillna(' ').copy()\n",
        "X_test = testdf.body_clean.fillna(' ').copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rp5h8fw9xsKH",
        "outputId": "67096141-9efa-49e5-e6af-6ac12812f9ce"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "model_name = 'bert-base-cased'\n",
        "max_length = 512\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "train_encodings = tokenizer(X_train.tolist(), truncation=True, padding=True, max_length=max_length, return_attention_mask=True)\n",
        "test_encodings  = tokenizer(X_test.tolist(), truncation=True, padding=True, max_length=max_length, return_attention_mask=True)\n",
        "\n",
        "train_dataset = MyDataset(train_encodings, y_train.tolist())\n",
        "test_dataset = MyDataset(test_encodings, y_test.tolist())\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2).to(device)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    num_train_epochs=3,              # total number of training epochs\n",
        "    per_device_train_batch_size=16,  # batch size per device during training\n",
        "    per_device_eval_batch_size=16,   # batch size for evaluation\n",
        "    learning_rate=2e-5,              # initial learning rate for Adam optimizer\n",
        "    warmup_steps=50,                # number of warmup steps for learning rate scheduler (set lower because of small dataset size)\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    output_dir='results',          # output directory\n",
        "    logging_dir='logs',            # directory for storing logs\n",
        "    logging_steps=100,               # number of steps to output logging (set lower because of small dataset size)\n",
        "    evaluation_strategy='steps',     # evaluate during fine-tuning so that we can see progress\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=train_dataset,         # training dataset\n",
        "    eval_dataset=test_dataset,           # evaluation dataset (usually a validation set; here we just send our test set)\n",
        "    compute_metrics=compute_metrics      # our custom evaluation function\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "id": "1eAA4m8gxsKI",
        "outputId": "01761d38-0f86-4c8c-d32d-e019ed941a84"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2814' max='2814' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2814/2814 35:01, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Precision Score</th>\n",
              "      <th>Accuracy Score</th>\n",
              "      <th>Recall Score</th>\n",
              "      <th>F1 Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.504000</td>\n",
              "      <td>0.401634</td>\n",
              "      <td>0.917124</td>\n",
              "      <td>0.834200</td>\n",
              "      <td>0.734800</td>\n",
              "      <td>0.815901</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.365600</td>\n",
              "      <td>0.323349</td>\n",
              "      <td>0.867264</td>\n",
              "      <td>0.877400</td>\n",
              "      <td>0.891200</td>\n",
              "      <td>0.879069</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.326700</td>\n",
              "      <td>0.306857</td>\n",
              "      <td>0.870229</td>\n",
              "      <td>0.888000</td>\n",
              "      <td>0.912000</td>\n",
              "      <td>0.890625</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.315500</td>\n",
              "      <td>0.302582</td>\n",
              "      <td>0.888181</td>\n",
              "      <td>0.890200</td>\n",
              "      <td>0.892800</td>\n",
              "      <td>0.890485</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.298400</td>\n",
              "      <td>0.307801</td>\n",
              "      <td>0.896940</td>\n",
              "      <td>0.894400</td>\n",
              "      <td>0.891200</td>\n",
              "      <td>0.894061</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.290600</td>\n",
              "      <td>0.283940</td>\n",
              "      <td>0.867263</td>\n",
              "      <td>0.894000</td>\n",
              "      <td>0.930400</td>\n",
              "      <td>0.897723</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.315400</td>\n",
              "      <td>0.272003</td>\n",
              "      <td>0.883712</td>\n",
              "      <td>0.898600</td>\n",
              "      <td>0.918000</td>\n",
              "      <td>0.900530</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.284300</td>\n",
              "      <td>0.287574</td>\n",
              "      <td>0.882443</td>\n",
              "      <td>0.898200</td>\n",
              "      <td>0.918800</td>\n",
              "      <td>0.900255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.277300</td>\n",
              "      <td>0.287935</td>\n",
              "      <td>0.891650</td>\n",
              "      <td>0.903400</td>\n",
              "      <td>0.918400</td>\n",
              "      <td>0.904828</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.244700</td>\n",
              "      <td>0.301779</td>\n",
              "      <td>0.867658</td>\n",
              "      <td>0.895600</td>\n",
              "      <td>0.933600</td>\n",
              "      <td>0.899422</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.232700</td>\n",
              "      <td>0.287655</td>\n",
              "      <td>0.899326</td>\n",
              "      <td>0.903000</td>\n",
              "      <td>0.907600</td>\n",
              "      <td>0.903444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.240700</td>\n",
              "      <td>0.265398</td>\n",
              "      <td>0.900040</td>\n",
              "      <td>0.903400</td>\n",
              "      <td>0.907600</td>\n",
              "      <td>0.903804</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>0.253100</td>\n",
              "      <td>0.288009</td>\n",
              "      <td>0.881614</td>\n",
              "      <td>0.901000</td>\n",
              "      <td>0.926400</td>\n",
              "      <td>0.903452</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.202200</td>\n",
              "      <td>0.302069</td>\n",
              "      <td>0.926325</td>\n",
              "      <td>0.895800</td>\n",
              "      <td>0.860000</td>\n",
              "      <td>0.891931</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.241000</td>\n",
              "      <td>0.300953</td>\n",
              "      <td>0.916944</td>\n",
              "      <td>0.901600</td>\n",
              "      <td>0.883200</td>\n",
              "      <td>0.899756</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>0.235600</td>\n",
              "      <td>0.275637</td>\n",
              "      <td>0.907990</td>\n",
              "      <td>0.904400</td>\n",
              "      <td>0.900000</td>\n",
              "      <td>0.903978</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1700</td>\n",
              "      <td>0.226000</td>\n",
              "      <td>0.308850</td>\n",
              "      <td>0.918019</td>\n",
              "      <td>0.901800</td>\n",
              "      <td>0.882400</td>\n",
              "      <td>0.899857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>0.234200</td>\n",
              "      <td>0.275486</td>\n",
              "      <td>0.899802</td>\n",
              "      <td>0.903800</td>\n",
              "      <td>0.908800</td>\n",
              "      <td>0.904279</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1900</td>\n",
              "      <td>0.206900</td>\n",
              "      <td>0.288612</td>\n",
              "      <td>0.905713</td>\n",
              "      <td>0.906200</td>\n",
              "      <td>0.906800</td>\n",
              "      <td>0.906256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.176900</td>\n",
              "      <td>0.337553</td>\n",
              "      <td>0.916806</td>\n",
              "      <td>0.898800</td>\n",
              "      <td>0.877200</td>\n",
              "      <td>0.896566</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2100</td>\n",
              "      <td>0.176900</td>\n",
              "      <td>0.337813</td>\n",
              "      <td>0.903329</td>\n",
              "      <td>0.902200</td>\n",
              "      <td>0.900800</td>\n",
              "      <td>0.902063</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2200</td>\n",
              "      <td>0.186500</td>\n",
              "      <td>0.341569</td>\n",
              "      <td>0.908867</td>\n",
              "      <td>0.898400</td>\n",
              "      <td>0.885600</td>\n",
              "      <td>0.897083</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2300</td>\n",
              "      <td>0.179700</td>\n",
              "      <td>0.345057</td>\n",
              "      <td>0.901606</td>\n",
              "      <td>0.900000</td>\n",
              "      <td>0.898000</td>\n",
              "      <td>0.899800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2400</td>\n",
              "      <td>0.182000</td>\n",
              "      <td>0.328968</td>\n",
              "      <td>0.901160</td>\n",
              "      <td>0.901000</td>\n",
              "      <td>0.900800</td>\n",
              "      <td>0.900980</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.175900</td>\n",
              "      <td>0.337165</td>\n",
              "      <td>0.909238</td>\n",
              "      <td>0.904000</td>\n",
              "      <td>0.897600</td>\n",
              "      <td>0.903382</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2600</td>\n",
              "      <td>0.168700</td>\n",
              "      <td>0.341846</td>\n",
              "      <td>0.913079</td>\n",
              "      <td>0.903000</td>\n",
              "      <td>0.890800</td>\n",
              "      <td>0.901802</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2700</td>\n",
              "      <td>0.150500</td>\n",
              "      <td>0.342695</td>\n",
              "      <td>0.904077</td>\n",
              "      <td>0.904400</td>\n",
              "      <td>0.904800</td>\n",
              "      <td>0.904438</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2800</td>\n",
              "      <td>0.161800</td>\n",
              "      <td>0.349565</td>\n",
              "      <td>0.910939</td>\n",
              "      <td>0.904200</td>\n",
              "      <td>0.896000</td>\n",
              "      <td>0.903408</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='313' max='313' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [313/313 00:36]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9037\n"
          ]
        }
      ],
      "source": [
        "trainer.train()\n",
        "results = trainer.evaluate()\n",
        "print(round(results['eval_f1_score'], 4))\n",
        "\n",
        "trainer.save_model('models/bert_model')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arSxIIGFoTrb"
      },
      "source": [
        "#### Metadata + body (start)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B85yOfDkoTrc"
      },
      "outputs": [],
      "source": [
        "X_train = df.metadata.copy() + df.body_clean.copy()\n",
        "X_test = testdf.metadata.copy() + testdf.body_clean.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMSqE13roTrd",
        "outputId": "46dc7f39-c114-413d-e597-0768bb0075ae"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "#BERT base Headline tokenizations\n",
        "model_name = 'bert-base-cased'\n",
        "max_length = 512\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "train_encodings = tokenizer(X_train.tolist(), truncation=True, padding=True, max_length=max_length, return_attention_mask=True)\n",
        "test_encodings  = tokenizer(X_test.tolist(), truncation=True, padding=True, max_length=max_length, return_attention_mask=True)\n",
        "\n",
        "train_dataset = MyDataset(train_encodings, y_train.tolist())\n",
        "test_dataset = MyDataset(test_encodings, y_test.tolist())\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2).to(device)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    num_train_epochs=3,              # total number of training epochs\n",
        "    per_device_train_batch_size=16,  # batch size per device during training\n",
        "    per_device_eval_batch_size=16,   # batch size for evaluation\n",
        "    learning_rate=2e-5,              # initial learning rate for Adam optimizer\n",
        "    warmup_steps=50,                # number of warmup steps for learning rate scheduler (set lower because of small dataset size)\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    output_dir='results',          # output directory\n",
        "    logging_dir='logs',            # directory for storing logs\n",
        "    logging_steps=100,               # number of steps to output logging (set lower because of small dataset size)\n",
        "    evaluation_strategy='steps',     # evaluate during fine-tuning so that we can see progress\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=train_dataset,         # training dataset\n",
        "    eval_dataset=test_dataset,           # evaluation dataset (usually a validation set; here we just send our test set)\n",
        "    compute_metrics=compute_metrics      # our custom evaluation function\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "g_v-6RbJoTre",
        "outputId": "69d99c18-361f-409c-c9b2-3297f8f09458"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3546' max='3546' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3546/3546 31:40, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Precision Score</th>\n",
              "      <th>Accuracy Score</th>\n",
              "      <th>Recall Score</th>\n",
              "      <th>F1 Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.543500</td>\n",
              "      <td>0.383792</td>\n",
              "      <td>0.811973</td>\n",
              "      <td>0.857619</td>\n",
              "      <td>0.926853</td>\n",
              "      <td>0.865618</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.338400</td>\n",
              "      <td>0.305672</td>\n",
              "      <td>0.842601</td>\n",
              "      <td>0.886667</td>\n",
              "      <td>0.948027</td>\n",
              "      <td>0.892210</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.308400</td>\n",
              "      <td>0.249658</td>\n",
              "      <td>0.916914</td>\n",
              "      <td>0.906667</td>\n",
              "      <td>0.892204</td>\n",
              "      <td>0.904390</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.272500</td>\n",
              "      <td>0.269684</td>\n",
              "      <td>0.918651</td>\n",
              "      <td>0.907143</td>\n",
              "      <td>0.891242</td>\n",
              "      <td>0.904739</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.272500</td>\n",
              "      <td>0.249066</td>\n",
              "      <td>0.943098</td>\n",
              "      <td>0.905714</td>\n",
              "      <td>0.861405</td>\n",
              "      <td>0.900402</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.269000</td>\n",
              "      <td>0.234824</td>\n",
              "      <td>0.936428</td>\n",
              "      <td>0.917143</td>\n",
              "      <td>0.893167</td>\n",
              "      <td>0.914286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.232900</td>\n",
              "      <td>0.282857</td>\n",
              "      <td>0.955240</td>\n",
              "      <td>0.902381</td>\n",
              "      <td>0.842156</td>\n",
              "      <td>0.895141</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.282600</td>\n",
              "      <td>0.234188</td>\n",
              "      <td>0.949738</td>\n",
              "      <td>0.914286</td>\n",
              "      <td>0.872955</td>\n",
              "      <td>0.909729</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.244600</td>\n",
              "      <td>0.208697</td>\n",
              "      <td>0.915033</td>\n",
              "      <td>0.928571</td>\n",
              "      <td>0.943215</td>\n",
              "      <td>0.928910</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.235700</td>\n",
              "      <td>0.207958</td>\n",
              "      <td>0.925785</td>\n",
              "      <td>0.931429</td>\n",
              "      <td>0.936477</td>\n",
              "      <td>0.931100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.222900</td>\n",
              "      <td>0.227540</td>\n",
              "      <td>0.949328</td>\n",
              "      <td>0.919048</td>\n",
              "      <td>0.883542</td>\n",
              "      <td>0.915254</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.222900</td>\n",
              "      <td>0.241809</td>\n",
              "      <td>0.896615</td>\n",
              "      <td>0.918095</td>\n",
              "      <td>0.943215</td>\n",
              "      <td>0.919325</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>0.194700</td>\n",
              "      <td>0.224060</td>\n",
              "      <td>0.939182</td>\n",
              "      <td>0.924762</td>\n",
              "      <td>0.906641</td>\n",
              "      <td>0.922625</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.211500</td>\n",
              "      <td>0.230386</td>\n",
              "      <td>0.941591</td>\n",
              "      <td>0.922857</td>\n",
              "      <td>0.899904</td>\n",
              "      <td>0.920276</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.193100</td>\n",
              "      <td>0.216951</td>\n",
              "      <td>0.906250</td>\n",
              "      <td>0.926190</td>\n",
              "      <td>0.948989</td>\n",
              "      <td>0.927127</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>0.203800</td>\n",
              "      <td>0.224108</td>\n",
              "      <td>0.917603</td>\n",
              "      <td>0.930000</td>\n",
              "      <td>0.943215</td>\n",
              "      <td>0.930233</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1700</td>\n",
              "      <td>0.182300</td>\n",
              "      <td>0.223455</td>\n",
              "      <td>0.949393</td>\n",
              "      <td>0.928095</td>\n",
              "      <td>0.902791</td>\n",
              "      <td>0.925506</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>0.196500</td>\n",
              "      <td>0.286330</td>\n",
              "      <td>0.855918</td>\n",
              "      <td>0.900476</td>\n",
              "      <td>0.960539</td>\n",
              "      <td>0.905215</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1900</td>\n",
              "      <td>0.197500</td>\n",
              "      <td>0.217986</td>\n",
              "      <td>0.937562</td>\n",
              "      <td>0.925714</td>\n",
              "      <td>0.910491</td>\n",
              "      <td>0.923828</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.207300</td>\n",
              "      <td>0.217665</td>\n",
              "      <td>0.921252</td>\n",
              "      <td>0.928095</td>\n",
              "      <td>0.934552</td>\n",
              "      <td>0.927855</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2100</td>\n",
              "      <td>0.200800</td>\n",
              "      <td>0.209330</td>\n",
              "      <td>0.913165</td>\n",
              "      <td>0.926667</td>\n",
              "      <td>0.941290</td>\n",
              "      <td>0.927014</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2200</td>\n",
              "      <td>0.187900</td>\n",
              "      <td>0.213771</td>\n",
              "      <td>0.936893</td>\n",
              "      <td>0.933810</td>\n",
              "      <td>0.928778</td>\n",
              "      <td>0.932818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2300</td>\n",
              "      <td>0.216100</td>\n",
              "      <td>0.200807</td>\n",
              "      <td>0.931796</td>\n",
              "      <td>0.933333</td>\n",
              "      <td>0.933590</td>\n",
              "      <td>0.932692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2400</td>\n",
              "      <td>0.179200</td>\n",
              "      <td>0.215362</td>\n",
              "      <td>0.931335</td>\n",
              "      <td>0.930000</td>\n",
              "      <td>0.926853</td>\n",
              "      <td>0.929088</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.158200</td>\n",
              "      <td>0.238010</td>\n",
              "      <td>0.934236</td>\n",
              "      <td>0.932857</td>\n",
              "      <td>0.929740</td>\n",
              "      <td>0.931983</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2600</td>\n",
              "      <td>0.182000</td>\n",
              "      <td>0.230387</td>\n",
              "      <td>0.934698</td>\n",
              "      <td>0.930000</td>\n",
              "      <td>0.923003</td>\n",
              "      <td>0.928814</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2700</td>\n",
              "      <td>0.135900</td>\n",
              "      <td>0.228682</td>\n",
              "      <td>0.941119</td>\n",
              "      <td>0.933333</td>\n",
              "      <td>0.923003</td>\n",
              "      <td>0.931973</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2800</td>\n",
              "      <td>0.149000</td>\n",
              "      <td>0.247112</td>\n",
              "      <td>0.934426</td>\n",
              "      <td>0.934286</td>\n",
              "      <td>0.932628</td>\n",
              "      <td>0.933526</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2900</td>\n",
              "      <td>0.198600</td>\n",
              "      <td>0.229258</td>\n",
              "      <td>0.920755</td>\n",
              "      <td>0.930000</td>\n",
              "      <td>0.939365</td>\n",
              "      <td>0.929967</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.129700</td>\n",
              "      <td>0.246985</td>\n",
              "      <td>0.924257</td>\n",
              "      <td>0.926667</td>\n",
              "      <td>0.927815</td>\n",
              "      <td>0.926033</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3100</td>\n",
              "      <td>0.129100</td>\n",
              "      <td>0.248323</td>\n",
              "      <td>0.926527</td>\n",
              "      <td>0.930952</td>\n",
              "      <td>0.934552</td>\n",
              "      <td>0.930522</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3200</td>\n",
              "      <td>0.125800</td>\n",
              "      <td>0.268295</td>\n",
              "      <td>0.919660</td>\n",
              "      <td>0.928095</td>\n",
              "      <td>0.936477</td>\n",
              "      <td>0.927992</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3300</td>\n",
              "      <td>0.143400</td>\n",
              "      <td>0.245293</td>\n",
              "      <td>0.932367</td>\n",
              "      <td>0.931429</td>\n",
              "      <td>0.928778</td>\n",
              "      <td>0.930569</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3400</td>\n",
              "      <td>0.142600</td>\n",
              "      <td>0.242031</td>\n",
              "      <td>0.936337</td>\n",
              "      <td>0.929524</td>\n",
              "      <td>0.920115</td>\n",
              "      <td>0.928155</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.161200</td>\n",
              "      <td>0.240924</td>\n",
              "      <td>0.935421</td>\n",
              "      <td>0.929048</td>\n",
              "      <td>0.920115</td>\n",
              "      <td>0.927705</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='132' max='132' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [132/132 00:15]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "trainer.train()\n",
        "results = trainer.evaluate()\n",
        "print(round(results['eval_f1_score'], 4))\n",
        "\n",
        "trainer.save_model('models/body_start_metadata')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1n0H6HX90jw9"
      },
      "source": [
        "#### Start of Article + End of Article"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NU6i-LkA0jw-"
      },
      "outputs": [],
      "source": [
        "X_train = df['start_end'].copy()\n",
        "X_test = testdf['start_end'].copy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDGb2N4T0jxA",
        "outputId": "bd7b4a73-79fd-401f-d933-aa7987d5f528"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "model_name = 'bert-base-cased'\n",
        "max_length = 512\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "train_encodings = tokenizer(X_train.tolist(), truncation=True, padding=True, max_length=max_length, return_attention_mask=True)\n",
        "test_encodings  = tokenizer(X_test.tolist(), truncation=True, padding=True, max_length=max_length, return_attention_mask=True)\n",
        "\n",
        "train_dataset = MyDataset(train_encodings, y_train.tolist())\n",
        "test_dataset = MyDataset(test_encodings, y_test.tolist())\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2).to(device)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    num_train_epochs=3,              # total number of training epochs\n",
        "    per_device_train_batch_size=16,  # batch size per device during training\n",
        "    per_device_eval_batch_size=16,   # batch size for evaluation\n",
        "    learning_rate=2e-5,              # initial learning rate for Adam optimizer\n",
        "    warmup_steps=50,                # number of warmup steps for learning rate scheduler (set lower because of small dataset size)\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    output_dir='results',          # output directory\n",
        "    logging_dir='logs',            # directory for storing logs\n",
        "    logging_steps=100,               # number of steps to output logging (set lower because of small dataset size)\n",
        "    evaluation_strategy='steps',     # evaluate during fine-tuning so that we can see progress\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=train_dataset,         # training dataset\n",
        "    eval_dataset=test_dataset,           # evaluation dataset (usually a validation set; here we just send our test set)\n",
        "    compute_metrics=compute_metrics      # our custom evaluation function\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 516
        },
        "id": "-6lENf6m0jxB",
        "outputId": "3123ef0d-ab01-42b4-8f48-621ef32d9500"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1182' max='1182' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1182/1182 08:28, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Precision Score</th>\n",
              "      <th>Accuracy Score</th>\n",
              "      <th>Recall Score</th>\n",
              "      <th>F1 Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.492600</td>\n",
              "      <td>0.337310</td>\n",
              "      <td>0.909968</td>\n",
              "      <td>0.884286</td>\n",
              "      <td>0.842262</td>\n",
              "      <td>0.874807</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.333800</td>\n",
              "      <td>0.295872</td>\n",
              "      <td>0.829016</td>\n",
              "      <td>0.882857</td>\n",
              "      <td>0.952381</td>\n",
              "      <td>0.886427</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.333900</td>\n",
              "      <td>0.267246</td>\n",
              "      <td>0.873596</td>\n",
              "      <td>0.900000</td>\n",
              "      <td>0.925595</td>\n",
              "      <td>0.898844</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.259400</td>\n",
              "      <td>0.233129</td>\n",
              "      <td>0.917889</td>\n",
              "      <td>0.927143</td>\n",
              "      <td>0.931548</td>\n",
              "      <td>0.924668</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.242000</td>\n",
              "      <td>0.248252</td>\n",
              "      <td>0.949045</td>\n",
              "      <td>0.922857</td>\n",
              "      <td>0.886905</td>\n",
              "      <td>0.916923</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.212500</td>\n",
              "      <td>0.239974</td>\n",
              "      <td>0.930723</td>\n",
              "      <td>0.928571</td>\n",
              "      <td>0.919643</td>\n",
              "      <td>0.925150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.225100</td>\n",
              "      <td>0.242518</td>\n",
              "      <td>0.960784</td>\n",
              "      <td>0.922857</td>\n",
              "      <td>0.875000</td>\n",
              "      <td>0.915888</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.214200</td>\n",
              "      <td>0.249626</td>\n",
              "      <td>0.954984</td>\n",
              "      <td>0.924286</td>\n",
              "      <td>0.883929</td>\n",
              "      <td>0.918083</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.179200</td>\n",
              "      <td>0.234548</td>\n",
              "      <td>0.935976</td>\n",
              "      <td>0.928571</td>\n",
              "      <td>0.913690</td>\n",
              "      <td>0.924699</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.161400</td>\n",
              "      <td>0.268475</td>\n",
              "      <td>0.928358</td>\n",
              "      <td>0.930000</td>\n",
              "      <td>0.925595</td>\n",
              "      <td>0.926975</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.172900</td>\n",
              "      <td>0.274502</td>\n",
              "      <td>0.923754</td>\n",
              "      <td>0.932857</td>\n",
              "      <td>0.937500</td>\n",
              "      <td>0.930576</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='44' max='44' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [44/44 00:05]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9261\n"
          ]
        }
      ],
      "source": [
        "trainer.train()\n",
        "results = trainer.evaluate()\n",
        "print(round(results['eval_f1_score'], 4))\n",
        "\n",
        "trainer.save_model('models/start_end')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMFQ5Mm8mTS1"
      },
      "source": [
        "#### Metadata + Start of article + End of article"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WOwfjk7VmTS3"
      },
      "outputs": [],
      "source": [
        "X_train = df['metadata'].copy() + '\\n****\\n' + df.start_end.copy()\n",
        "X_test = testdf['metadata'].copy() + '\\n****\\n' + testdf.start_end.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfB4eu7MmTS4",
        "outputId": "7090e6ac-76d6-4351-ae54-c0bc137d88b7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "#BERT base Headline tokenizations\n",
        "model_name = 'bert-base-cased'\n",
        "max_length = 512\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "train_encodings = tokenizer(X_train.tolist(), truncation=True, padding=True, max_length=max_length, return_attention_mask=True)\n",
        "test_encodings  = tokenizer(X_test.tolist(), truncation=True, padding=True, max_length=max_length, return_attention_mask=True)\n",
        "\n",
        "train_dataset = MyDataset(train_encodings, y_train.tolist())\n",
        "test_dataset = MyDataset(test_encodings, y_test.tolist())\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2).to(device)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    num_train_epochs=3,              # total number of training epochs\n",
        "    per_device_train_batch_size=16,  # batch size per device during training\n",
        "    per_device_eval_batch_size=16,   # batch size for evaluation\n",
        "    learning_rate=2e-5,              # initial learning rate for Adam optimizer\n",
        "    warmup_steps=50,                # number of warmup steps for learning rate scheduler (set lower because of small dataset size)\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    output_dir='results',          # output directory\n",
        "    logging_dir='logs',            # directory for storing logs\n",
        "    logging_steps=100,               # number of steps to output logging (set lower because of small dataset size)\n",
        "    evaluation_strategy='steps',     # evaluate during fine-tuning so that we can see progress\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=train_dataset,         # training dataset\n",
        "    eval_dataset=test_dataset,           # evaluation dataset (usually a validation set; here we just send our test set)\n",
        "    compute_metrics=compute_metrics      # our custom evaluation function\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 516
        },
        "id": "-chrN6HfmTS6",
        "outputId": "ae34a9c0-25c0-4203-d576-e837e78681ed"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1875' max='1875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1875/1875 16:17, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Precision Score</th>\n",
              "      <th>Accuracy Score</th>\n",
              "      <th>Recall Score</th>\n",
              "      <th>F1 Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.500900</td>\n",
              "      <td>0.297612</td>\n",
              "      <td>0.881207</td>\n",
              "      <td>0.891500</td>\n",
              "      <td>0.905000</td>\n",
              "      <td>0.892945</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.348800</td>\n",
              "      <td>0.278995</td>\n",
              "      <td>0.866917</td>\n",
              "      <td>0.891500</td>\n",
              "      <td>0.925000</td>\n",
              "      <td>0.895017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.280900</td>\n",
              "      <td>0.250098</td>\n",
              "      <td>0.905567</td>\n",
              "      <td>0.908000</td>\n",
              "      <td>0.911000</td>\n",
              "      <td>0.908275</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.290300</td>\n",
              "      <td>0.231334</td>\n",
              "      <td>0.910872</td>\n",
              "      <td>0.919500</td>\n",
              "      <td>0.930000</td>\n",
              "      <td>0.920336</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.240400</td>\n",
              "      <td>0.256250</td>\n",
              "      <td>0.923780</td>\n",
              "      <td>0.917000</td>\n",
              "      <td>0.909000</td>\n",
              "      <td>0.916331</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.248800</td>\n",
              "      <td>0.254912</td>\n",
              "      <td>0.943677</td>\n",
              "      <td>0.917500</td>\n",
              "      <td>0.888000</td>\n",
              "      <td>0.914992</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.211000</td>\n",
              "      <td>0.234576</td>\n",
              "      <td>0.924303</td>\n",
              "      <td>0.926000</td>\n",
              "      <td>0.928000</td>\n",
              "      <td>0.926148</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.187100</td>\n",
              "      <td>0.280454</td>\n",
              "      <td>0.890244</td>\n",
              "      <td>0.916000</td>\n",
              "      <td>0.949000</td>\n",
              "      <td>0.918683</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.202200</td>\n",
              "      <td>0.264564</td>\n",
              "      <td>0.929293</td>\n",
              "      <td>0.925000</td>\n",
              "      <td>0.920000</td>\n",
              "      <td>0.924623</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.198600</td>\n",
              "      <td>0.255331</td>\n",
              "      <td>0.931034</td>\n",
              "      <td>0.925000</td>\n",
              "      <td>0.918000</td>\n",
              "      <td>0.924471</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.192900</td>\n",
              "      <td>0.242820</td>\n",
              "      <td>0.906040</td>\n",
              "      <td>0.923500</td>\n",
              "      <td>0.945000</td>\n",
              "      <td>0.925110</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.195200</td>\n",
              "      <td>0.230136</td>\n",
              "      <td>0.926295</td>\n",
              "      <td>0.928000</td>\n",
              "      <td>0.930000</td>\n",
              "      <td>0.928144</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>0.171500</td>\n",
              "      <td>0.294962</td>\n",
              "      <td>0.899905</td>\n",
              "      <td>0.919500</td>\n",
              "      <td>0.944000</td>\n",
              "      <td>0.921425</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.152300</td>\n",
              "      <td>0.315742</td>\n",
              "      <td>0.886492</td>\n",
              "      <td>0.912000</td>\n",
              "      <td>0.945000</td>\n",
              "      <td>0.914811</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.132400</td>\n",
              "      <td>0.331680</td>\n",
              "      <td>0.890566</td>\n",
              "      <td>0.914000</td>\n",
              "      <td>0.944000</td>\n",
              "      <td>0.916505</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>0.127400</td>\n",
              "      <td>0.290266</td>\n",
              "      <td>0.930792</td>\n",
              "      <td>0.929500</td>\n",
              "      <td>0.928000</td>\n",
              "      <td>0.929394</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1700</td>\n",
              "      <td>0.147400</td>\n",
              "      <td>0.285646</td>\n",
              "      <td>0.921337</td>\n",
              "      <td>0.928500</td>\n",
              "      <td>0.937000</td>\n",
              "      <td>0.929103</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>0.159000</td>\n",
              "      <td>0.264619</td>\n",
              "      <td>0.922167</td>\n",
              "      <td>0.928500</td>\n",
              "      <td>0.936000</td>\n",
              "      <td>0.929032</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [125/125 00:14]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9303\n"
          ]
        }
      ],
      "source": [
        "trainer.train()\n",
        "results = trainer.evaluate()\n",
        "print(round(results['eval_f1_score'], 4))\n",
        "\n",
        "trainer.save_model('models/metadata_start_end')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzSV3v1pljze"
      },
      "source": [
        "#### End of article + metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "rMcicCi4ljzg"
      },
      "outputs": [],
      "source": [
        "X_train = df.Body.fillna(' ').copy() + '\\n*****\\n' + df['metadata'].copy()\n",
        "X_test = testdf.Body.fillna(' ').copy() + '\\n*****\\n' + testdf['metadata'].copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "fV88oM8Lljzh",
        "outputId": "1b1e4f24-5849-46fc-84db-946ad68465e6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "#BERT base Headline tokenizations\n",
        "model_name = 'bert-base-cased'\n",
        "max_length = 512\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name, truncation_side='left')\n",
        "train_encodings = tokenizer(X_train.tolist(), truncation=True, padding=True, max_length=max_length, return_attention_mask=True)\n",
        "test_encodings  = tokenizer(X_test.tolist(), truncation=True, padding=True, max_length=max_length, return_attention_mask=True)\n",
        "\n",
        "train_dataset = MyDataset(train_encodings, y_train.tolist())\n",
        "test_dataset = MyDataset(test_encodings, y_test.tolist())\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2).to(device)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    num_train_epochs=3,              # total number of training epochs\n",
        "    per_device_train_batch_size=16,  # batch size per device during training\n",
        "    per_device_eval_batch_size=16,   # batch size for evaluation\n",
        "    learning_rate=2e-5,              # initial learning rate for Adam optimizer\n",
        "    warmup_steps=50,                # number of warmup steps for learning rate scheduler (set lower because of small dataset size)\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    output_dir='results',          # output directory\n",
        "    logging_dir='logs',            # directory for storing logs\n",
        "    logging_steps=100,               # number of steps to output logging (set lower because of small dataset size)\n",
        "    evaluation_strategy='steps',     # evaluate during fine-tuning so that we can see progress\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=train_dataset,         # training dataset\n",
        "    eval_dataset=test_dataset,           # evaluation dataset (usually a validation set; here we just send our test set)\n",
        "    compute_metrics=compute_metrics      # our custom evaluation function\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 516
        },
        "id": "leR0xS0Bljzj",
        "outputId": "ec0d99ca-e6b6-45ad-fa3e-a98894bd322a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1875' max='1875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1875/1875 16:19, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Precision Score</th>\n",
              "      <th>Accuracy Score</th>\n",
              "      <th>Recall Score</th>\n",
              "      <th>F1 Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.527200</td>\n",
              "      <td>0.327592</td>\n",
              "      <td>0.893861</td>\n",
              "      <td>0.878500</td>\n",
              "      <td>0.859000</td>\n",
              "      <td>0.876084</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.366300</td>\n",
              "      <td>0.528618</td>\n",
              "      <td>0.755521</td>\n",
              "      <td>0.824000</td>\n",
              "      <td>0.958000</td>\n",
              "      <td>0.844797</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.363500</td>\n",
              "      <td>0.293525</td>\n",
              "      <td>0.929032</td>\n",
              "      <td>0.899000</td>\n",
              "      <td>0.864000</td>\n",
              "      <td>0.895337</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.327400</td>\n",
              "      <td>0.291343</td>\n",
              "      <td>0.846709</td>\n",
              "      <td>0.884500</td>\n",
              "      <td>0.939000</td>\n",
              "      <td>0.890469</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.288400</td>\n",
              "      <td>0.280945</td>\n",
              "      <td>0.899804</td>\n",
              "      <td>0.907000</td>\n",
              "      <td>0.916000</td>\n",
              "      <td>0.907830</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.270000</td>\n",
              "      <td>0.269364</td>\n",
              "      <td>0.918605</td>\n",
              "      <td>0.896000</td>\n",
              "      <td>0.869000</td>\n",
              "      <td>0.893114</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.228000</td>\n",
              "      <td>0.283780</td>\n",
              "      <td>0.927505</td>\n",
              "      <td>0.901000</td>\n",
              "      <td>0.870000</td>\n",
              "      <td>0.897833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.226400</td>\n",
              "      <td>0.306506</td>\n",
              "      <td>0.906439</td>\n",
              "      <td>0.904000</td>\n",
              "      <td>0.901000</td>\n",
              "      <td>0.903711</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.235400</td>\n",
              "      <td>0.250528</td>\n",
              "      <td>0.910803</td>\n",
              "      <td>0.914500</td>\n",
              "      <td>0.919000</td>\n",
              "      <td>0.914883</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.228000</td>\n",
              "      <td>0.296777</td>\n",
              "      <td>0.926349</td>\n",
              "      <td>0.911000</td>\n",
              "      <td>0.893000</td>\n",
              "      <td>0.909369</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.226400</td>\n",
              "      <td>0.293373</td>\n",
              "      <td>0.880597</td>\n",
              "      <td>0.908000</td>\n",
              "      <td>0.944000</td>\n",
              "      <td>0.911197</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.230800</td>\n",
              "      <td>0.259424</td>\n",
              "      <td>0.903036</td>\n",
              "      <td>0.911500</td>\n",
              "      <td>0.922000</td>\n",
              "      <td>0.912420</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>0.194100</td>\n",
              "      <td>0.284141</td>\n",
              "      <td>0.894129</td>\n",
              "      <td>0.909500</td>\n",
              "      <td>0.929000</td>\n",
              "      <td>0.911231</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.198100</td>\n",
              "      <td>0.276895</td>\n",
              "      <td>0.908555</td>\n",
              "      <td>0.915500</td>\n",
              "      <td>0.924000</td>\n",
              "      <td>0.916212</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.173100</td>\n",
              "      <td>0.291666</td>\n",
              "      <td>0.911000</td>\n",
              "      <td>0.911000</td>\n",
              "      <td>0.911000</td>\n",
              "      <td>0.911000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>0.163800</td>\n",
              "      <td>0.287199</td>\n",
              "      <td>0.903320</td>\n",
              "      <td>0.913000</td>\n",
              "      <td>0.925000</td>\n",
              "      <td>0.914032</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1700</td>\n",
              "      <td>0.166100</td>\n",
              "      <td>0.291156</td>\n",
              "      <td>0.905790</td>\n",
              "      <td>0.913500</td>\n",
              "      <td>0.923000</td>\n",
              "      <td>0.914314</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>0.185800</td>\n",
              "      <td>0.272747</td>\n",
              "      <td>0.918182</td>\n",
              "      <td>0.914000</td>\n",
              "      <td>0.909000</td>\n",
              "      <td>0.913568</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [125/125 00:14]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9128\n"
          ]
        }
      ],
      "source": [
        "trainer.train()\n",
        "results = trainer.evaluate()\n",
        "print(round(results['eval_f1_score'], 4))\n",
        "\n",
        "trainer.save_model('models/end_metadata')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVSZrGOMeAsm"
      },
      "source": [
        "#### End of article"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4Dw5CH6neAsn"
      },
      "outputs": [],
      "source": [
        "X_train = df.Body.fillna(' ').copy()\n",
        "X_test = testdf.Body.fillna(' ').copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ias-IekJeAso",
        "outputId": "fd01fab7-e795-4835-f60a-a2a6169df3ee"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "#BERT base Headline tokenizations\n",
        "model_name = 'bert-base-cased'\n",
        "max_length = 512\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name, truncation_side='left')\n",
        "train_encodings = tokenizer(X_train.tolist(), truncation=True, padding=True, max_length=max_length, return_attention_mask=True)\n",
        "test_encodings  = tokenizer(X_test.tolist(), truncation=True, padding=True, max_length=max_length, return_attention_mask=True)\n",
        "\n",
        "train_dataset = MyDataset(train_encodings, y_train.tolist())\n",
        "test_dataset = MyDataset(test_encodings, y_test.tolist())\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2).to(device)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    num_train_epochs=3,              # total number of training epochs\n",
        "    per_device_train_batch_size=16,  # batch size per device during training\n",
        "    per_device_eval_batch_size=16,   # batch size for evaluation\n",
        "    learning_rate=2e-5,              # initial learning rate for Adam optimizer\n",
        "    warmup_steps=50,                # number of warmup steps for learning rate scheduler (set lower because of small dataset size)\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    output_dir='results',          # output directory\n",
        "    logging_dir='logs',            # directory for storing logs\n",
        "    logging_steps=100,               # number of steps to output logging (set lower because of small dataset size)\n",
        "    evaluation_strategy='steps',     # evaluate during fine-tuning so that we can see progress\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=train_dataset,         # training dataset\n",
        "    eval_dataset=test_dataset,           # evaluation dataset (usually a validation set; here we just send our test set)\n",
        "    compute_metrics=compute_metrics      # our custom evaluation function\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "id": "o-BaHQ7LeAsp",
        "outputId": "63ff1d77-e6ba-4f62-83b8-5d894ace314c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='829' max='1875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 829/1875 29:57 < 37:53, 0.46 it/s, Epoch 1.32/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Precision Score</th>\n",
              "      <th>Accuracy Score</th>\n",
              "      <th>Recall Score</th>\n",
              "      <th>F1 Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.520700</td>\n",
              "      <td>0.348769</td>\n",
              "      <td>0.908267</td>\n",
              "      <td>0.860500</td>\n",
              "      <td>0.802000</td>\n",
              "      <td>0.851832</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.397000</td>\n",
              "      <td>0.293673</td>\n",
              "      <td>0.893145</td>\n",
              "      <td>0.890000</td>\n",
              "      <td>0.886000</td>\n",
              "      <td>0.889558</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.291700</td>\n",
              "      <td>0.280139</td>\n",
              "      <td>0.881553</td>\n",
              "      <td>0.893000</td>\n",
              "      <td>0.908000</td>\n",
              "      <td>0.894581</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.307700</td>\n",
              "      <td>0.283502</td>\n",
              "      <td>0.905102</td>\n",
              "      <td>0.897000</td>\n",
              "      <td>0.887000</td>\n",
              "      <td>0.895960</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.275500</td>\n",
              "      <td>0.285982</td>\n",
              "      <td>0.930283</td>\n",
              "      <td>0.895000</td>\n",
              "      <td>0.854000</td>\n",
              "      <td>0.890511</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.272100</td>\n",
              "      <td>0.329055</td>\n",
              "      <td>0.940698</td>\n",
              "      <td>0.879000</td>\n",
              "      <td>0.809000</td>\n",
              "      <td>0.869892</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.227600</td>\n",
              "      <td>0.338202</td>\n",
              "      <td>0.935484</td>\n",
              "      <td>0.878000</td>\n",
              "      <td>0.812000</td>\n",
              "      <td>0.869379</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.224800</td>\n",
              "      <td>0.315236</td>\n",
              "      <td>0.916842</td>\n",
              "      <td>0.896000</td>\n",
              "      <td>0.871000</td>\n",
              "      <td>0.893333</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "trainer.train()\n",
        "results = trainer.evaluate()\n",
        "print(round(results['eval_f1_score'], 4))\n",
        "\n",
        "trainer.save_model('models/end')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktT5DBFXbMRn"
      },
      "source": [
        "#### Metadata, max_length=128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6RmlamfNbMRo"
      },
      "outputs": [],
      "source": [
        "X_train = df.metadata.fillna(' ').copy()\n",
        "X_test = testdf.metadata.fillna(' ').copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgl8eZYNbMRp",
        "outputId": "4bccc2f2-3321-48c2-aa83-158bea6bd4b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "#BERT base Headline tokenizations\n",
        "model_name = 'bert-base-cased'\n",
        "max_length = 128\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "train_encodings = tokenizer(X_train.tolist(), truncation=True, padding=True, max_length=max_length, return_attention_mask=True)\n",
        "test_encodings  = tokenizer(X_test.tolist(), truncation=True, padding=True, max_length=max_length, return_attention_mask=True)\n",
        "\n",
        "train_dataset = MyDataset(train_encodings, y_train.tolist())\n",
        "test_dataset = MyDataset(test_encodings, y_test.tolist())\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2).to(device)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    num_train_epochs=3,              # total number of training epochs\n",
        "    per_device_train_batch_size=16,  # batch size per device during training\n",
        "    per_device_eval_batch_size=16,   # batch size for evaluation\n",
        "    learning_rate=2e-5,              # initial learning rate for Adam optimizer\n",
        "    warmup_steps=50,                # number of warmup steps for learning rate scheduler (set lower because of small dataset size)\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    output_dir='results',          # output directory\n",
        "    logging_dir='logs',            # directory for storing logs\n",
        "    logging_steps=100,               # number of steps to output logging (set lower because of small dataset size)\n",
        "    evaluation_strategy='steps',     # evaluate during fine-tuning so that we can see progress\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=train_dataset,         # training dataset\n",
        "    eval_dataset=test_dataset,           # evaluation dataset (usually a validation set; here we just send our test set)\n",
        "    compute_metrics=compute_metrics      # our custom evaluation function\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "96k03SzrbMRq",
        "outputId": "5517254b-cfaa-4a09-dca0-25981d559fb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2814' max='2814' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2814/2814 08:41, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Precision Score</th>\n",
              "      <th>Accuracy Score</th>\n",
              "      <th>Recall Score</th>\n",
              "      <th>F1 Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.578500</td>\n",
              "      <td>0.454434</td>\n",
              "      <td>0.758598</td>\n",
              "      <td>0.809800</td>\n",
              "      <td>0.908800</td>\n",
              "      <td>0.826934</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.385800</td>\n",
              "      <td>0.370990</td>\n",
              "      <td>0.829140</td>\n",
              "      <td>0.850600</td>\n",
              "      <td>0.883200</td>\n",
              "      <td>0.855317</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.345400</td>\n",
              "      <td>0.373734</td>\n",
              "      <td>0.777043</td>\n",
              "      <td>0.835000</td>\n",
              "      <td>0.939600</td>\n",
              "      <td>0.850625</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.317700</td>\n",
              "      <td>0.313906</td>\n",
              "      <td>0.859076</td>\n",
              "      <td>0.860800</td>\n",
              "      <td>0.863200</td>\n",
              "      <td>0.861133</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.320100</td>\n",
              "      <td>0.367132</td>\n",
              "      <td>0.804393</td>\n",
              "      <td>0.854800</td>\n",
              "      <td>0.937600</td>\n",
              "      <td>0.865903</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.320400</td>\n",
              "      <td>0.322070</td>\n",
              "      <td>0.805337</td>\n",
              "      <td>0.857000</td>\n",
              "      <td>0.941600</td>\n",
              "      <td>0.868154</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.334800</td>\n",
              "      <td>0.301197</td>\n",
              "      <td>0.865265</td>\n",
              "      <td>0.875200</td>\n",
              "      <td>0.888800</td>\n",
              "      <td>0.876875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.307000</td>\n",
              "      <td>0.310657</td>\n",
              "      <td>0.842494</td>\n",
              "      <td>0.871400</td>\n",
              "      <td>0.913600</td>\n",
              "      <td>0.876607</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.305400</td>\n",
              "      <td>0.308670</td>\n",
              "      <td>0.903129</td>\n",
              "      <td>0.876200</td>\n",
              "      <td>0.842800</td>\n",
              "      <td>0.871922</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.242400</td>\n",
              "      <td>0.366881</td>\n",
              "      <td>0.890968</td>\n",
              "      <td>0.884400</td>\n",
              "      <td>0.876000</td>\n",
              "      <td>0.883421</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.238200</td>\n",
              "      <td>0.316333</td>\n",
              "      <td>0.899156</td>\n",
              "      <td>0.878400</td>\n",
              "      <td>0.852400</td>\n",
              "      <td>0.875154</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.227100</td>\n",
              "      <td>0.327690</td>\n",
              "      <td>0.849683</td>\n",
              "      <td>0.875000</td>\n",
              "      <td>0.911200</td>\n",
              "      <td>0.879367</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>0.235300</td>\n",
              "      <td>0.321220</td>\n",
              "      <td>0.843265</td>\n",
              "      <td>0.880200</td>\n",
              "      <td>0.934000</td>\n",
              "      <td>0.886316</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.223300</td>\n",
              "      <td>0.305209</td>\n",
              "      <td>0.841024</td>\n",
              "      <td>0.878400</td>\n",
              "      <td>0.933200</td>\n",
              "      <td>0.884717</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.219100</td>\n",
              "      <td>0.337232</td>\n",
              "      <td>0.858578</td>\n",
              "      <td>0.885400</td>\n",
              "      <td>0.922800</td>\n",
              "      <td>0.889532</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>0.237200</td>\n",
              "      <td>0.298990</td>\n",
              "      <td>0.884509</td>\n",
              "      <td>0.890200</td>\n",
              "      <td>0.897600</td>\n",
              "      <td>0.891007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1700</td>\n",
              "      <td>0.220700</td>\n",
              "      <td>0.341770</td>\n",
              "      <td>0.919691</td>\n",
              "      <td>0.870000</td>\n",
              "      <td>0.810800</td>\n",
              "      <td>0.861820</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>0.205400</td>\n",
              "      <td>0.346923</td>\n",
              "      <td>0.853289</td>\n",
              "      <td>0.882400</td>\n",
              "      <td>0.923600</td>\n",
              "      <td>0.887053</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1900</td>\n",
              "      <td>0.225000</td>\n",
              "      <td>0.356108</td>\n",
              "      <td>0.859445</td>\n",
              "      <td>0.883600</td>\n",
              "      <td>0.917200</td>\n",
              "      <td>0.887384</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.154800</td>\n",
              "      <td>0.384255</td>\n",
              "      <td>0.891508</td>\n",
              "      <td>0.885400</td>\n",
              "      <td>0.877600</td>\n",
              "      <td>0.884499</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2100</td>\n",
              "      <td>0.165200</td>\n",
              "      <td>0.412153</td>\n",
              "      <td>0.895382</td>\n",
              "      <td>0.887000</td>\n",
              "      <td>0.876400</td>\n",
              "      <td>0.885789</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2200</td>\n",
              "      <td>0.170200</td>\n",
              "      <td>0.371731</td>\n",
              "      <td>0.882120</td>\n",
              "      <td>0.886400</td>\n",
              "      <td>0.892000</td>\n",
              "      <td>0.887033</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2300</td>\n",
              "      <td>0.155600</td>\n",
              "      <td>0.421228</td>\n",
              "      <td>0.881589</td>\n",
              "      <td>0.888000</td>\n",
              "      <td>0.896400</td>\n",
              "      <td>0.888933</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2400</td>\n",
              "      <td>0.162000</td>\n",
              "      <td>0.414905</td>\n",
              "      <td>0.886545</td>\n",
              "      <td>0.888400</td>\n",
              "      <td>0.890800</td>\n",
              "      <td>0.888667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.151000</td>\n",
              "      <td>0.417088</td>\n",
              "      <td>0.874566</td>\n",
              "      <td>0.888200</td>\n",
              "      <td>0.906400</td>\n",
              "      <td>0.890198</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2600</td>\n",
              "      <td>0.154600</td>\n",
              "      <td>0.404126</td>\n",
              "      <td>0.885135</td>\n",
              "      <td>0.887600</td>\n",
              "      <td>0.890800</td>\n",
              "      <td>0.887959</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2700</td>\n",
              "      <td>0.125200</td>\n",
              "      <td>0.423330</td>\n",
              "      <td>0.882748</td>\n",
              "      <td>0.887800</td>\n",
              "      <td>0.894400</td>\n",
              "      <td>0.888536</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2800</td>\n",
              "      <td>0.165000</td>\n",
              "      <td>0.418656</td>\n",
              "      <td>0.883096</td>\n",
              "      <td>0.888000</td>\n",
              "      <td>0.894400</td>\n",
              "      <td>0.888712</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='313' max='313' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [313/313 00:08]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8887\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
              "\u001b[31m│\u001b[0m in \u001b[92m<cell line: 9>\u001b[0m:\u001b[94m9\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/\u001b[0m\u001b[1;33mserialization.py\u001b[0m:\u001b[94m440\u001b[0m in \u001b[92msave\u001b[0m                       \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 437 \u001b[0m\u001b[2m│   \u001b[0m_check_save_filelike(f)                                                               \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 438 \u001b[0m\u001b[2m│   \u001b[0m                                                                                      \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 439 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mif\u001b[0m _use_new_zipfile_serialization:                                                    \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 440 \u001b[2m│   │   \u001b[0m\u001b[94mwith\u001b[0m _open_zipfile_writer(f) \u001b[94mas\u001b[0m opened_zipfile:                                   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 441 \u001b[0m\u001b[2m│   │   │   \u001b[0m_save(obj, opened_zipfile, pickle_module, pickle_protocol)                    \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 442 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m                                                                        \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 443 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94melse\u001b[0m:                                                                                 \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/\u001b[0m\u001b[1;33mserialization.py\u001b[0m:\u001b[94m315\u001b[0m in \u001b[92m_open_zipfile_writer\u001b[0m       \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 312 \u001b[0m\u001b[2m│   │   \u001b[0mcontainer = _open_zipfile_writer_file                                             \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 313 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94melse\u001b[0m:                                                                                 \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 314 \u001b[0m\u001b[2m│   │   \u001b[0mcontainer = _open_zipfile_writer_buffer                                           \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 315 \u001b[2m│   \u001b[0m\u001b[94mreturn\u001b[0m container(name_or_buffer)                                                      \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 316 \u001b[0m                                                                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 317 \u001b[0m                                                                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 318 \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m_is_compressed_file\u001b[0m(f) -> \u001b[96mbool\u001b[0m:                                                       \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/\u001b[0m\u001b[1;33mserialization.py\u001b[0m:\u001b[94m288\u001b[0m in \u001b[92m__init__\u001b[0m                   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 285 \u001b[0m                                                                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 286 \u001b[0m\u001b[94mclass\u001b[0m \u001b[4;92m_open_zipfile_writer_file\u001b[0m(_opener):                                                 \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 287 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m__init__\u001b[0m(\u001b[96mself\u001b[0m, name) -> \u001b[94mNone\u001b[0m:                                                     \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 288 \u001b[2m│   │   \u001b[0m\u001b[96msuper\u001b[0m().\u001b[92m__init__\u001b[0m(torch._C.PyTorchFileWriter(\u001b[96mstr\u001b[0m(name)))                           \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 289 \u001b[0m\u001b[2m│   \u001b[0m                                                                                      \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 290 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m__exit__\u001b[0m(\u001b[96mself\u001b[0m, *args) -> \u001b[94mNone\u001b[0m:                                                    \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 291 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.file_like.write_end_of_file()                                                \u001b[31m│\u001b[0m\n",
              "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
              "\u001b[1;91mRuntimeError: \u001b[0mParent directory \u001b[35m/content/gdrive/\u001b[0m\u001b[95mMy\u001b[0m Drive/models does not exist.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;cell line: 9&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">9</span>                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/torch/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">serialization.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">440</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">save</span>                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 437 │   </span>_check_save_filelike(f)                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 438 │   </span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 439 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> _use_new_zipfile_serialization:                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 440 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> _open_zipfile_writer(f) <span style=\"color: #0000ff; text-decoration-color: #0000ff\">as</span> opened_zipfile:                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 441 │   │   │   </span>_save(obj, opened_zipfile, pickle_module, pickle_protocol)                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 442 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span>                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 443 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/torch/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">serialization.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">315</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_open_zipfile_writer</span>       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 312 │   │   </span>container = _open_zipfile_writer_file                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 313 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 314 │   │   </span>container = _open_zipfile_writer_buffer                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 315 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> container(name_or_buffer)                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 316 </span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 317 </span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 318 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_is_compressed_file</span>(f) -&gt; <span style=\"color: #00ffff; text-decoration-color: #00ffff\">bool</span>:                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/torch/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">serialization.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">288</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__init__</span>                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 285 </span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 286 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">class</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; text-decoration: underline\">_open_zipfile_writer_file</span>(_opener):                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 287 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__init__</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, name) -&gt; <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 288 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">super</span>().<span style=\"color: #00ff00; text-decoration-color: #00ff00\">__init__</span>(torch._C.PyTorchFileWriter(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">str</span>(name)))                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 289 │   </span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 290 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__exit__</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, *args) -&gt; <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 291 │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.file_like.write_end_of_file()                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
              "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">RuntimeError: </span>Parent directory <span style=\"color: #800080; text-decoration-color: #800080\">/content/gdrive/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">My</span> Drive/models does not exist.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "trainer.train()\n",
        "results = trainer.evaluate()\n",
        "print(round(results['eval_f1_score'], 4))\n",
        "\n",
        "trainer.save_model('models/metadata')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_save_name = 'metadata.pt'\n",
        "path = F\"/content/drive/MyDrive/{model_save_name}\"\n",
        "torch.save(model.state_dict(), path)"
      ],
      "metadata": {
        "id": "ijr1I6pUBndY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model('/content/drive/MyDrive/models/metadata')"
      ],
      "metadata": {
        "id": "swpX6LOjB_-t",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "outputId": "86f560d2-4252-4ba1-e355-29c74ef7219d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-237833214387>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/models/metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPx_lhowYFk6"
      },
      "source": [
        "#### Headline, max_length=64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ouiDFQnTYFk7"
      },
      "outputs": [],
      "source": [
        "#Pre-process Headline text using texthero\n",
        "X_train = df['Headline'].fillna('').copy()\n",
        "X_test = testdf['Headline'].fillna('').copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7mwQkdxYFk8",
        "outputId": "4caca4f4-c2c4-4802-d321-e971596364fb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "#BERT base Headline tokenizations\n",
        "model_name = 'bert-base-cased'\n",
        "max_length = 64\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "train_encodings = tokenizer(X_train.tolist(), truncation=True, padding=True, max_length=max_length, return_attention_mask=True)\n",
        "test_encodings  = tokenizer(X_test.tolist(), truncation=True, padding=True, max_length=max_length, return_attention_mask=True)\n",
        "\n",
        "train_dataset = MyDataset(train_encodings, y_train.tolist())\n",
        "test_dataset = MyDataset(test_encodings, y_test.tolist())\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2).to(device)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    num_train_epochs=3,              # total number of training epochs\n",
        "    per_device_train_batch_size=16,  # batch size per device during training\n",
        "    per_device_eval_batch_size=16,   # batch size for evaluation\n",
        "    learning_rate=2e-5,              # initial learning rate for Adam optimizer\n",
        "    warmup_steps=50,                # number of warmup steps for learning rate scheduler (set lower because of small dataset size)\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    output_dir='results',          # output directory\n",
        "    logging_dir='logs',            # directory for storing logs\n",
        "    logging_steps=100,               # number of steps to output logging (set lower because of small dataset size)\n",
        "    evaluation_strategy='steps',     # evaluate during fine-tuning so that we can see progress\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=train_dataset,         # training dataset\n",
        "    eval_dataset=test_dataset,           # evaluation dataset (usually a validation set; here we just send our test set)\n",
        "    compute_metrics=compute_metrics      # our custom evaluation function\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "id": "lGoE75yFYFk9",
        "outputId": "886705af-7473-4734-a625-378c2d20c220"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1182' max='1182' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1182/1182 04:28, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Precision Score</th>\n",
              "      <th>Accuracy Score</th>\n",
              "      <th>Recall Score</th>\n",
              "      <th>F1 Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.652500</td>\n",
              "      <td>0.504761</td>\n",
              "      <td>0.683146</td>\n",
              "      <td>0.752857</td>\n",
              "      <td>0.904762</td>\n",
              "      <td>0.778489</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.429800</td>\n",
              "      <td>0.412854</td>\n",
              "      <td>0.817910</td>\n",
              "      <td>0.824286</td>\n",
              "      <td>0.815476</td>\n",
              "      <td>0.816692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.410800</td>\n",
              "      <td>0.383790</td>\n",
              "      <td>0.775457</td>\n",
              "      <td>0.821429</td>\n",
              "      <td>0.883929</td>\n",
              "      <td>0.826147</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.351200</td>\n",
              "      <td>0.383127</td>\n",
              "      <td>0.792000</td>\n",
              "      <td>0.832857</td>\n",
              "      <td>0.883929</td>\n",
              "      <td>0.835443</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.271700</td>\n",
              "      <td>0.437273</td>\n",
              "      <td>0.865204</td>\n",
              "      <td>0.852857</td>\n",
              "      <td>0.821429</td>\n",
              "      <td>0.842748</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.294500</td>\n",
              "      <td>0.390629</td>\n",
              "      <td>0.806540</td>\n",
              "      <td>0.841429</td>\n",
              "      <td>0.880952</td>\n",
              "      <td>0.842105</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.274000</td>\n",
              "      <td>0.405914</td>\n",
              "      <td>0.849693</td>\n",
              "      <td>0.845714</td>\n",
              "      <td>0.824405</td>\n",
              "      <td>0.836858</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.252000</td>\n",
              "      <td>0.387320</td>\n",
              "      <td>0.854545</td>\n",
              "      <td>0.854286</td>\n",
              "      <td>0.839286</td>\n",
              "      <td>0.846847</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.194300</td>\n",
              "      <td>0.432231</td>\n",
              "      <td>0.868098</td>\n",
              "      <td>0.862857</td>\n",
              "      <td>0.842262</td>\n",
              "      <td>0.854985</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.176100</td>\n",
              "      <td>0.462393</td>\n",
              "      <td>0.852507</td>\n",
              "      <td>0.861429</td>\n",
              "      <td>0.860119</td>\n",
              "      <td>0.856296</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.177200</td>\n",
              "      <td>0.473625</td>\n",
              "      <td>0.857567</td>\n",
              "      <td>0.864286</td>\n",
              "      <td>0.860119</td>\n",
              "      <td>0.858841</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='44' max='44' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [44/44 00:02]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "trainer.train()\n",
        "results = trainer.evaluate()\n",
        "print(round(results['eval_f1_score'], 4))\n",
        "\n",
        "trainer.save_model('models/bert_headline_model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HbLlFUhaupfa"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RpBO9PnnupiW"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuClass": "premium"
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}